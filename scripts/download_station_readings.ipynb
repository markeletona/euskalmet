{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae02cfc5",
   "metadata": {},
   "source": [
    "## Download Euskalmet station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c875917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load required libraries\n",
    "\n",
    "# To create a token to be sent along all communications with the Euskalmet API:\n",
    "import jwt\n",
    "\n",
    "# To communicate with the Euskalmet API:\n",
    "import requests\n",
    "\n",
    "# To handle output of requests:\n",
    "import json\n",
    "\n",
    "# Data handling:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b2e0cd",
   "metadata": {},
   "source": [
    "Create my JWT (https://jwt.io/) based on instructions by Euskalmet (https://opendata.euskadi.eus/api-euskalmet/-/how-to-use-meteo-rest-services/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45edc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load payload information:\n",
    "fpath = \"../api/payload.txt\"\n",
    "f = open(fpath, 'r')\n",
    "payload = json.loads(f.read())\n",
    "\n",
    "## payload has the following structure:\n",
    "# payload = {\n",
    "#             \"aud\": \"met01.apikey\", # -> fixed value\n",
    "#             \"iss\": \"NAME\", # issuer description, whatever\n",
    "#             \"exp\": 1906453456, # expiration timestamp, as epoch. You can get the epoch for a date here -> https://www.epochconverter.com/\n",
    "#             \"version\": \"1.0.0\", # -> fixed value\n",
    "#             \"iat\": 1685455334, # emission timestamp, as epoch. Must be exp > iat\n",
    "#             \"email\": \"YOUR_EMAIL@EMAIL.COM\" # the email you used when asking for the API keys\n",
    "#         }\n",
    "\n",
    "# Create token based on payload and private key:\n",
    "my_jwt = jwt.encode(payload, open(\"../api/Apikey/privateKey.pem\", \"rb\").read(), algorithm = \"RS256\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee415afa",
   "metadata": {},
   "source": [
    "Based on this, create the header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eca2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"Bearer {my_jwt}\", \"Accept\": \"application/json\"}\n",
    "#headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e6629",
   "metadata": {},
   "source": [
    "Before starting to interact with the API it might be useful to save the string of the API url (the \"root\", so to speak) and avoid typing all of it every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cddd716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rurl = \"https://api.euskadi.eus\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b3642",
   "metadata": {},
   "source": [
    "Try downloading station list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99edfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(rurl + \"/euskalmet/stations\", headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef77077",
   "metadata": {},
   "source": [
    "We can see the fields within the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7eb2634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dir(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62fc47",
   "metadata": {},
   "source": [
    "The actual response to our request is within the _text_ field. We can handle the output with `json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3db4fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json.loads(r.text)\n",
    "with open(\"../data/station_info_list.json\", \"w\") as f:\n",
    "    f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99b5ec",
   "metadata": {},
   "source": [
    "This provides the list of stations. I've realised that there are several instances (snapshots) for each station. In each of them some sensor information might have changed, and that's very important when requesting the data, or you'll end up with 404 errors because you were asking for a sensor that was not installed in a certain date. (hmm but I can't retrieve information for all of them, seems weird...)\n",
    "\n",
    "Usually, we would be interested in one or a few stations. We can easily check the codes here: https://www.euskalmet.euskadi.eus/behaketa/estazioen-datuak/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c7f78",
   "metadata": {},
   "source": [
    "To start working with any station we need to know which sensors are available so that we can retrieve their readings:\n",
    "\n",
    "IMPORTANT: digging more into this I've seen that the (temperature) sensor happened on 2021-04-13-00. \n",
    "Dates >= 2021-04-13-00 have the new sensor.\n",
    "This does not match any of the dates I see in the station information?? I mean, the sensor information from \n",
    "the snapshots seems correct, but I have had to guess the date of the change...\n",
    "Anyway, I will need to consider it, but this is problematic would I want to automatise the process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71297f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select station:\n",
    "station_id = \"C068\"\n",
    "snapshots = [\"20150220\", \"20221003\"] # YYYYMMDD format\n",
    "sensors_info = []\n",
    "\n",
    "for snp in snapshots:\n",
    "    # Retrieve information:\n",
    "    addurl = f\"/euskalmet/stations/{station_id}/{snp}\"\n",
    "    r_sensors = requests.get(rurl + addurl, headers = headers)\n",
    "    info = json.loads(r_sensors.text)\n",
    "    \n",
    "    # Extract the sensor IDs (stored within the \"sensors\" field):\n",
    "    sensor_ids = [x[\"sensorKey\"].split(\"/\")[-1] for x in info[\"sensors\"]]\n",
    "\n",
    "    # Iterate through sensors, obtaining the 'measureType' and 'measureId' of each variable (they are stored within the \"meteors\" field)\n",
    "    sensors_info.append({x: json.loads(requests.get(rurl + f\"/euskalmet/sensors/{x}\", headers = headers).text)[\"meteors\"] for x in sensor_ids})\n",
    "\n",
    "# Note that in 'sensors' the -> 'unit': 'CENTIMETERS' <- refers to the height at which the sensor is located in the station.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62037f5c",
   "metadata": {},
   "source": [
    "Now set the time window in which we want to download data.\n",
    "\n",
    "We want to download the entire dataset of the station, but doing so every time we run the script is not efficient.\n",
    "So, set a condition by which we check if the readings output file of this specific station already exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5651555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for station C068 already exists. Reading last date to continue download until today.\n"
     ]
    }
   ],
   "source": [
    "fpath = f\"../data/readings_{station_id}.csv\"\n",
    "if os.path.exists(fpath):\n",
    "    # if it does, check which was the last date, and set the range from inmediately after to now:\n",
    "    # read file line by line and print the last line\n",
    "    # (this is probably not efficient for large files, so I might need to come back to change it)\n",
    "    print(f\"File for station {station_id} already exists. Reading last date to continue download until today.\")\n",
    "    with open(fpath,'r') as file:        \n",
    "            for line in file:\n",
    "                pass\n",
    "    last_date = [float(x) for x in line.replace('\\n', '').split(\",\")]\n",
    "    start_date = pd.Timestamp(year = int(last_date[0]), \n",
    "                              month = int(last_date[1]), \n",
    "                              day = int(last_date[2]), \n",
    "                              hour = int(last_date[3])+1, \n",
    "                              tz = \"utc\")\n",
    "    end_date = pd.Timestamp.now(tz = \"utc\")\n",
    "else:\n",
    "    # if it does not, create date range from beginning to now:\n",
    "    # To get all the data available, set the start date to the installation date of the station (available in the sensor information):\n",
    "    print(f\"No file exists for station {station_id}. Setting dates to download full dataset.\")\n",
    "    start_date = pd.to_datetime(info['installDate'], format = \"%Y-%m-%dT%H:%M:%S\", utc = True) # format according to ISO 8601\n",
    "    end_date = pd.Timestamp.now(tz = \"utc\")\n",
    "    \n",
    "\n",
    "# Create the desired date range to retrieve data:\n",
    "dater = pd.date_range(start = start_date, end = end_date, freq = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401856b",
   "metadata": {},
   "source": [
    "Iterate through all the measurements and dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46642c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create named dataframes to stores temporary and full results:\n",
    "#\n",
    "# Get the variable names for our station so that we add them as column names:\n",
    "sensors_vars = [[d['measureId'] for d in values] for sensor_id, values in sensors_info[0].items()]\n",
    "sensors_vars = [item for sublist in sensors_vars for item in sublist] # the output is a list of lists, this merges them\n",
    "colnames = ['YYYY', 'MM', 'DD', 'hh', 'mm'] + sensors_vars + [\"api_status\"]\n",
    "tmp = pd.DataFrame(columns = colnames)\n",
    "\n",
    "# Create csv file with header if not already created:\n",
    "fpath = f\"../data/readings_{station_id}.csv\"\n",
    "if not os.path.exists(fpath):\n",
    "    tmp.to_csv(fpath, na_rep = \"nan\", header = True, index = False)\n",
    "    \n",
    "# To get all the readings, we will need to loop through dates, sensors and measurents:\n",
    "for date_i in dater:\n",
    "    tmp['YYYY'] = np.repeat(date_i.year, 6)\n",
    "    tmp['MM'] = np.repeat(date_i.month, 6)\n",
    "    tmp['DD'] = np.repeat(date_i.day, 6)\n",
    "    tmp['hh'] = np.repeat(date_i.hour, 6)\n",
    "    tmp['mm'] = np.arange(0, 60, 10) # arange() considers the start and stop of the interval as [start, stop) so to get 50 I need to set stop=60\n",
    "    \n",
    "    # Optional, to keep track of the download process for large batches:\n",
    "    #with open('../data/output.txt', 'w') as f:\n",
    "    #    f.write(f'Downloading: {date_i}')\n",
    "    \n",
    "    # As aforementioned, we need to consider the date we are in to get the correct sensor information:\n",
    "    if date_i < pd.Timestamp(year = 2021, month = 4, day = 13, hour = 0, tz = \"utc\"):\n",
    "        si = sensors_info[0].items()\n",
    "    else:\n",
    "        si = sensors_info[1].items()\n",
    "        \n",
    "    for sensor_id, values in si:\n",
    "        for measure_type in values:\n",
    "\n",
    "            # Get the measure type and measure id of this iteration:\n",
    "            measure_type_id = measure_type[\"measureType\"]\n",
    "            measure_id = measure_type[\"measureId\"]\n",
    "\n",
    "            # Create the url for the request based on these, plus the date:\n",
    "            url = (rurl +\n",
    "                   f\"/euskalmet/readings/forStation/{station_id}/{sensor_id}/measures/{measure_type_id}/{measure_id}/\"\n",
    "                   f\"at/{int(date_i.year):04}/{int(date_i.month):02}/{int(date_i.day):02}/{int(date_i.hour):02}\")\n",
    "\n",
    "            # Send request to get readings:\n",
    "            rd = requests.get(url, headers = headers)\n",
    "\n",
    "            # Check status_code of response (>=300 means trouble):\n",
    "            if rd.status_code<300:\n",
    "                # If OK, get the data:\n",
    "                tmp[f\"{measure_id}\"] = json.loads(rd.text)['values'] # store values of the specific variable\n",
    "                tmp[\"api_status\"] = np.repeat(rd.status_code, 6)\n",
    "\n",
    "            else:\n",
    "                # If not, fill with nan:\n",
    "                tmp[f\"{measure_id}\"] = np.repeat(np.nan, 6)\n",
    "                tmp[\"api_status\"] = np.repeat(rd.status_code, 6)\n",
    "\n",
    "\n",
    "    # Append readings from this iteration to the full table:\n",
    "    # There is a shit-ton of data and sending-receiving requests to the Euskalment API looks like the bottleneck.\n",
    "    # I've estimated that for around 10 years of data it will take ~72h. So I think it will be better to append results\n",
    "    # from every iteration to the csv file. This way, as I always check the last day of the file to resume the download,\n",
    "    # I can do it in batches. It's also really helpful when the internet connection fails and the script stops, otherwise \n",
    "    # I would need to restarts from scratch if I do not save the progress continuously (has happenned before, hehe).\n",
    "    tmp.to_csv(fpath, mode = 'a', na_rep = \"nan\", index = False, header = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
